import pandas as pd
import pandas_profiling as pdp
import multiprocessing
from tqdm import tqdm
import setting
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split, StratifiedKFold, KFold
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
from datetime import datetime
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt

dtypes = setting.DTYPES
ID = 'MachineIdentifier'
TARGET = 'HasDetections'
DATA_DIR_PATH = 'data/'
PROFILE_DIR_PATH = 'profile_report/'
SUBMIT_DIR_PATH = 'submit/'


def load_dataframe(dataset):
    usecols = dtypes.keys()
    if dataset == 'test':
        usecols = [col for col in dtypes.keys() if col != 'HasDetections']
    df = pd.read_csv(f'data/{dataset}.csv', dtype=dtypes, usecols=usecols, header=0)
    return df


def check_filepath(filepath):
    return os.path.isfile(filepath)


def output_profile(df, filename):
    filepath = PROFILE_DIR_PATH + filename
    if check_filepath(filepath):
        print(f'already exist file path <{filepath}>')
    else:
        print(f'create profiling report <{filepath}>')
        profile = pdp.ProfileReport(df)
        profile.to_file(filepath)


def xgb_analysis(X, y):
    clf = xgb.XGBClassifier(n_estimators=1000,
                            learning_rate=0.1,
                            max_depth=7,
                            min_child_weight=1,
                            gamma=0,
                            subsample=0.8,
                            colsample_bytree=0.8,
                            scale_pos_weight=1,
                            objective='binary:logistic',
                            nthread=8,
                            # silent=False
                            )
    # clf = RandomForestClassifier(n_jobs=-1, verbose=3)
    xgb_param = clf.get_xgb_params()
    xgb_train = xgb.DMatrix(X.values, label=y.values)
    cv_result = xgb.cv(xgb_param, xgb_train, num_boost_round=xgb_param['n_estimators'], nfold=5,
                       metrics='auc', early_stopping_rounds=50)
    clf.set_params(n_estimators=cv_result.shape[0])
    clf.fit(X, y, eval_metric='auc')
    return clf, cv_result.shape[0]


def lgb_analysis(X, y):
    print('LGB Start')
    clf = lgb.LGBMClassifier(n_estimators=1000,
                             learning_rate=0.1,
                             max_depth=7,
                             min_child_weight=1,
                             subsample=0.1,
                             colsample_bytree=0.8,
                             # objective='binary:logistic',
                             n_jobs=-1)
    lgb_param = clf.get_params()
    lgb_train = lgb.Dataset(X, y)
    cv_result = lgb.cv(lgb_param, lgb_train, nfold=5,
                       metrics='auc', early_stopping_rounds=50)
    print(cv_result)
    clf.fit(X, y, eval_metric='auc')
    return clf


if __name__ == '__main__':
    with multiprocessing.Pool() as pool:
        df_train, df_test = pool.map(load_dataframe, ["train", "test"])
    # df_train = load_dataframe('train')

    output_profile(df_train, 'train.html')
    output_profile(df_test, 'test.html')

    df_train = df_train.sample(1000)
    df_test = df_test.sample(1000)

    # print(df_train.info())
    feat_cols = [x for x in df_train.columns if x not in [TARGET, ID]]

    # y_train = pd.get_dummies(y_train)

    df_train.fillna(df_train.median(), inplace=True)
    # x_train.replace('<', '', inplace=True)
    le = LabelEncoder()

    x_dtypes = df_train[feat_cols].dtypes
    for column in x_dtypes[x_dtypes == 'object'].index:
        le = LabelEncoder()
        df_train[column] = le.fit_transform(df_train[column].astype(str))
        df_test[column] = le.fit_transform(df_test[column].astype(str))

    df_train[feat_cols] = MinMaxScaler().fit_transform(df_train[feat_cols])
    df_test[feat_cols] = MinMaxScaler().fit_transform(df_test[feat_cols])

    # print(df_train[feat_cols])

    x_train, x_valid, y_train, y_valid = train_test_split(df_train[feat_cols], df_train[TARGET], test_size=0.2,
                                                          random_state=0)

    # skf = StratifiedKFold()
    #
    # for train, test in skf.split(df_train[feat_cols], df_train[TARGET]):

    print('x train shape:', x_train.shape)
    print('x valid shape:', x_valid.shape)
    print('y train shape:', y_train.shape)
    print('y valid shape:', y_valid.shape)

    xgb_clf, ntree_limit = xgb_analysis(x_train, y_train)
    lgb_clf = lgb_analysis(x_train, y_train)

    xgb_pred = xgb_clf.predict(x_valid, ntree_limit=ntree_limit)
    df_xgb_pred = pd.Series(xgb_pred)

    lgb_pred = xgb_clf.predict(x_valid)
    df_lgb_pred = pd.Series(lgb_pred)

    df_pred = pd.concat([df_xgb_pred, df_lgb_pred], axis=1)
    df_pred['mean'] = df_pred.mean(axis=1)

    auc_score = roc_auc_score(y_valid, df_pred['mean'])

    print(accuracy_score(y_valid, df_xgb_pred))
    print(classification_report(y_valid, df_xgb_pred))
    print(auc_score)

    df_xgb_feat_imp = pd.Series(xgb_clf.feature_importances_, index=feat_cols)
    df_lgb_feat_imp = pd.Series(lgb_clf.feature_importances_, index=feat_cols)
    df_feat_imp = pd.concat([df_xgb_pred, df_lgb_pred], axis=1)
    df_feat_imp['mean'] = df_feat_imp.mean(axis=1)
    print(df_feat_imp.sort_values(ascending=False, by='mean'))

    xgb_pred_proba = xgb_clf.predict_proba(df_test[feat_cols])
    lgb_pred_proba = lgb_clf.predict_proba(df_test[feat_cols])
    df_xgb_proba = pd.Series(xgb_pred_proba[:, 1], index=df_test[ID], name='xgb proba')
    df_lgb_proba = pd.Series(lgb_pred_proba[:, 1], index=df_test[ID], name='lgb proba')

    df_proba = pd.concat([df_xgb_proba, df_lgb_proba], axis=1)
    df_proba[TARGET] = df_proba.mean(axis=1)
    # print(df_proba)

    str_nowtime = datetime.now().strftime("%Y%m%d%H%M%S")
    df_proba[TARGET].to_csv(SUBMIT_DIR_PATH + f'submit_{str_nowtime}_{round(auc_score * 100, 0)}.csv', header=True)
